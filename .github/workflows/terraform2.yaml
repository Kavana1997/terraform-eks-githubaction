name: Deploy EKS with Terraform

on:
  push:
    branches: [ main ]
    paths:
      - '**.tf'
      - '.github/workflows/terraform.yml'
  pull_request:
    branches: [ main ]
    paths:
      - '**.tf'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod

env:
  TF_VAR_environment: ${{ github.event.inputs.environment || 'dev' }}
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}
  S3_BUCKET_NAME: terraform-eks-state-bucket-${{ github.repository_owner }}-${{ github.event.repository.name }}
  DYNAMODB_TABLE_NAME: terraform-state-lock
  WORKSPACE: ${{ github.event.inputs.environment || github.ref_name || 'dev' }}

jobs:
  terraform:
    name: Terraform
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    # Step 1: Prepare backend infrastructure
    - name: Prepare Terraform Backend
      run: |
        # Create S3 bucket for terraform state if it doesn't exist
        if ! aws s3 ls "s3://${S3_BUCKET_NAME}" 2>&1 > /dev/null; then
          echo "Creating S3 bucket for Terraform state: ${S3_BUCKET_NAME}"
          aws s3api create-bucket \
            --bucket ${S3_BUCKET_NAME} \
            --region ${AWS_REGION} \
            $([[ "${AWS_REGION}" != "us-east-1" ]] && echo "--create-bucket-configuration LocationConstraint=${AWS_REGION}")
          
          echo "Enabling versioning for S3 bucket"
          aws s3api put-bucket-versioning \
            --bucket ${S3_BUCKET_NAME} \
            --versioning-configuration Status=Enabled
          
          echo "Enabling encryption for S3 bucket"
          aws s3api put-bucket-encryption \
            --bucket ${S3_BUCKET_NAME} \
            --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
        else
          echo "S3 bucket already exists: ${S3_BUCKET_NAME}"
        fi
        
        # Create DynamoDB table for state locking if it doesn't exist
        if ! aws dynamodb describe-table --table-name ${DYNAMODB_TABLE_NAME} 2>&1 > /dev/null; then
          echo "Creating DynamoDB table for state locking: ${DYNAMODB_TABLE_NAME}"
          aws dynamodb create-table \
            --table-name ${DYNAMODB_TABLE_NAME} \
            --attribute-definitions AttributeName=LockID,AttributeType=S \
            --key-schema AttributeName=LockID,KeyType=HASH \
            --billing-mode PAY_PER_REQUEST
        else
          echo "DynamoDB table already exists: ${DYNAMODB_TABLE_NAME}"
        fi
        
        # Create backend.tf file with correct configuration
        cat > backend.tf << EOF
        terraform {
          backend "s3" {
            bucket         = "${S3_BUCKET_NAME}"
            key            = "terraform/eks/${WORKSPACE}/terraform.tfstate"
            region         = "${AWS_REGION}"
            encrypt        = true
            dynamodb_table = "${DYNAMODB_TABLE_NAME}"
          }
        }
        EOF

    # Step 2: Check AWS resource limits
    - name: Check AWS Resource Limits
      run: |
        # Check VPC limit
        VPC_COUNT=$(aws ec2 describe-vpcs --query 'length(Vpcs)' --output text)
        VPC_LIMIT=$(aws service-quotas get-service-quota --service-code ec2 --quota-code L-F678F1CE --query 'Quota.Value' --output text 2>/dev/null || echo 5)
        
        echo "Current VPC count: $VPC_COUNT (Limit: $VPC_LIMIT)"
        
        if [ "$VPC_COUNT" -ge "$VPC_LIMIT" ]; then
          echo "‚ö†Ô∏è WARNING: VPC limit reached ($VPC_COUNT/$VPC_LIMIT). Looking for unused VPCs to clean up..."
          
          # List existing VPCs
          echo "Existing VPCs:"
          aws ec2 describe-vpcs --query 'Vpcs[].{ID:VpcId,CIDR:CidrBlock,Name:Tags[?Key==`Name`].Value|[0]}' --output table
          
          # Look for unused VPCs (no instances running)
          UNUSED_VPCS=$(aws ec2 describe-vpcs --query 'Vpcs[?!length(Tags[?Key==`Name` && contains(Value, `eks`) || contains(Value, `EKS`)])].[VpcId]' --output text)
          
          if [ -n "$UNUSED_VPCS" ]; then
            echo "Found potentially unused VPCs: $UNUSED_VPCS"
            echo "You may want to delete these manually if they are not needed"
            # Uncomment to allow automatic deletion (USE WITH CAUTION):
            # for VPC in $UNUSED_VPCS; do
            #   echo "Attempting to delete VPC: $VPC"
            #   aws ec2 delete-vpc --vpc-id $VPC || echo "Could not delete VPC $VPC automatically"
            # done
          fi
        fi

    # Step 3: Initialize Terraform
    - name: Terraform Init
      id: init
      run: terraform init

    # Step 4: Check for existing resources and import them
    - name: Check and Import Existing Resources
      id: import
      run: |
        # Extract resource names from Terraform files
        S3_BUCKET_NAME_TF=$(grep -o 'resource "aws_s3_bucket" "[^"]*"' *.tf | grep -o '"[^"]*"$' | tr -d '"' || echo "")
        CLUSTER_ROLE_NAME_TF=$(grep -o 'name[[:space:]]*=[[:space:]]*"[^"]*-cluster-role[^"]*"' *.tf | grep -o '"[^"]*"' | tr -d '"' || echo "")
        NODE_ROLE_NAME_TF=$(grep -o 'name[[:space:]]*=[[:space:]]*"[^"]*-node-role[^"]*"' *.tf | grep -o '"[^"]*"' | tr -d '"' || echo "")
        
        echo "S3 Bucket from TF: $S3_BUCKET_NAME_TF"
        echo "Cluster Role from TF: $CLUSTER_ROLE_NAME_TF"
        echo "Node Role from TF: $NODE_ROLE_NAME_TF"
        
        # Import S3 bucket if it exists
        if [ -n "$S3_BUCKET_NAME_TF" ]; then
          if aws s3 ls "s3://$S3_BUCKET_NAME_TF" 2>/dev/null; then
            echo "S3 bucket $S3_BUCKET_NAME_TF exists, checking if it's in state..."
            if ! terraform state list | grep -q "aws_s3_bucket.$S3_BUCKET_NAME_TF"; then
              echo "Importing S3 bucket $S3_BUCKET_NAME_TF into Terraform state..."
              terraform import aws_s3_bucket.terraform_state1 $S3_BUCKET_NAME_TF || echo "Failed to import S3 bucket"
            else
              echo "S3 bucket already in Terraform state"
            fi
          fi
        fi
        
        # Import cluster role if it exists
        if [ -n "$CLUSTER_ROLE_NAME_TF" ]; then
          if aws iam get-role --role-name "$CLUSTER_ROLE_NAME_TF" 2>/dev/null; then
            echo "Cluster role $CLUSTER_ROLE_NAME_TF exists, checking if it's in state..."
            if ! terraform state list | grep -q "aws_iam_role.eks_cluster_role"; then
              echo "Importing cluster role $CLUSTER_ROLE_NAME_TF into Terraform state..."
              terraform import aws_iam_role.eks_cluster_role $CLUSTER_ROLE_NAME_TF || echo "Failed to import cluster role"
            else
              echo "Cluster role already in Terraform state"
            fi
          fi
        fi
        
        # Import node role if it exists
        if [ -n "$NODE_ROLE_NAME_TF" ]; then
          if aws iam get-role --role-name "$NODE_ROLE_NAME_TF" 2>/dev/null; then
            echo "Node role $NODE_ROLE_NAME_TF exists, checking if it's in state..."
            if ! terraform state list | grep -q "aws_iam_role.eks_node_role"; then
              echo "Importing node role $NODE_ROLE_NAME_TF into Terraform state..."
              terraform import aws_iam_role.eks_node_role $NODE_ROLE_NAME_TF || echo "Failed to import node role"
            else
              echo "Node role already in Terraform state"
            fi
          fi
        fi

    # Step 5: Validate Terraform configuration
    - name: Terraform Validate
      id: validate
      run: terraform validate -no-color

    # Step 6: Format Terraform code
    - name: Terraform Format
      id: fmt
      run: terraform fmt -check
      continue-on-error: true

    # Step 7: Create Terraform plan
    - name: Terraform Plan
      id: plan
      run: terraform plan -no-color -input=false -out=tfplan
      continue-on-error: true

    # Step 8: Update Pull Request with plan results
    - name: Update Pull Request
      uses: actions/github-script@v7
      if: github.event_name == 'pull_request'
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const output = `#### Terraform Format and Style üñå\`${{ steps.fmt.outcome }}\`
          #### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
          #### Terraform Validation ü§ñ\`${{ steps.validate.outcome }}\`
          #### Terraform Plan üìñ\`${{ steps.plan.outcome }}\`

          <details><summary>Show Plan</summary>

          \`\`\`terraform
          ${process.env.PLAN}
          \`\`\`

          </details>`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: output
          });
      env:
        PLAN: "${{ steps.plan.outputs.stdout }}"

    # Step 9: Apply Terraform changes (only on main branch push or manual workflow dispatch)
    - name: Terraform Apply
      if: (github.ref == 'refs/heads/main' && github.event_name == 'push') || github.event_name == 'workflow_dispatch'
      run: terraform apply -auto-approve tfplan
      timeout-minutes: 30

    # Step 10: Get EKS Kubeconfig (only if apply was successful)
    - name: Get Kubeconfig
      if: (github.ref == 'refs/heads/main' && github.event_name == 'push') || github.event_name == 'workflow_dispatch'
      run: |
        CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "")
        
        if [ -n "$CLUSTER_NAME" ]; then
          echo "Updating kubeconfig for cluster: $CLUSTER_NAME"
          aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_REGION}
          
          echo "Checking cluster nodes:"
          kubectl get nodes
        else
          echo "No cluster name output found in Terraform state"
        fi

  # Optional job to clean up resources
  cleanup:
    name: Cleanup Resources
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.cleanup == 'true'
    needs: terraform
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0
          
      - name: Terraform Init
        run: terraform init
        
      - name: Terraform Destroy
        run: terraform destroy -auto-approve
        timeout-minutes: 30
